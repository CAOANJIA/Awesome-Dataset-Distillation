{
  "2025/02/05": [
  {
    "title": "Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training",
    "author": "Xunxin Cai & Chengrui Wang & Qingqing Long et al., DASFAA 2025",
    "github": null,
    "url": "https://arxiv.org/abs/2501.15108",
    "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/cai2025llm.txt",
    "website": null
  },
  {
    "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation",
    "author": "Huimin Lu et al., ICLR 2025",
    "github": null,
    "url": "https://openreview.net/forum?id=eLLBILFRsA",
    "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/lu2025llm.txt",
    "website": null
  },
  {
    "title": "Distilling Reinforcement Learning into Single-Batch Datasets",
    "author": "Connor Wilhelm et al., ICLR 2025",
    "github": null,
    "url": "https://openreview.net/forum?id=XnX7xRoroC",
    "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/wilhelm2025rl.txt",
    "website": null
  },
  {
    "title": "Influence-Guided Diffusion for Dataset Distillation",
    "author": "Mingyang Chen et al., ICLR 2025",
    "github": null,
    "url": "https://openreview.net/forum?id=0whx8MhysK",
    "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/chen2025igd.txt",
    "website": null
  },
  {
    "title": "Distilling Dataset into Neural Field",
    "author": "Donghyeok Shin et al., ICLR 2025",
    "github": null,
    "url": "https://openreview.net/forum?id=nCrJD7qPJN",
    "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/shin2025ddif.txt",
    "website": null
  },
  {
    "title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation",
    "author": "Sheng-Feng Yu et al., ICLR 2025",
    "github": null,
    "url": "https://openreview.net/forum?id=2RfWRKwxYh",
    "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/yu2025self.txt",
    "website": null
  }
],
  "2025/01/23": [
    {
      "title": "Going Beyond Feature Similarity: Effective Dataset Distillation based on Class-aware Conditional Mutual Information",
      "author": "Xinhao Zhong et al., ICLR 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2412.09945",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/zhong2025cmi.txt",
      "website": null
    },
    {
      "title": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator",
      "author": "Xin Zhang et al., ICLR 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2408.06927",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/zhang2025infer.txt",
      "website": null
    },
    {
      "title": "Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-Training of Deep Networks",
      "author": "Siddharth Joshi et al., ICLR 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2410.02116",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/joshi2025kd.txt",
      "website": null
    },
    {
      "title": "Group Distributionally Robust Dataset Distillation with Risk Minimization",
      "author": "Saeed Vahidian & Mingyu Wang & Jianyang Gu et al., ICLR 2025",
      "github": "https://github.com/Mming11/RobustDatasetDistillation",
      "url": "https://arxiv.org/abs/2402.04676",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/vahidian2025group.txt",
      "website": null
    }
  ]
}