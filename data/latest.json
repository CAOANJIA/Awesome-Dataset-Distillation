{
  "2025/03/10": [
    {
      "title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation",
      "author": "Zhiqiang Shen & Ammar Sherif et al., CVPR 2025",
      "github": "https://github.com/VILA-Lab/DELT",
      "url": "https://arxiv.org/abs/2411.19946",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/shen2025delt.txt",
      "website": null
    }
  ],
  "2025/03/07": [
    {
      "title": "Hierarchical Features Matter: A Deep Exploration of GAN Priors for Improved Dataset Distillation",
      "author": "Xinhao Zhong & Hao Fang et al., CVPR 2025",
      "github": "https://github.com/ndhg1213/H-GLaD",
      "url": "https://arxiv.org/abs/2406.05704",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/zhong2025hglad.txt",
      "website": null
    }
  ],
  "2025/03/04": [
    {
      "title": "Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified Trajectory",
      "author": "Wenliang Zhong et al., CVPR 2025",
      "github": "https://github.githubassets.com/images/icons/emoji/octocat.png",
      "url": "https://arxiv.org/abs/2406.19827",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/zhong2025mct.txt",
      "website": null
    },
    {
      "title": "Dataset Distillation with Neural Characteristic Function: A Minmax Perspective",
      "author": "Shaobo Wang et al., CVPR 2025",
      "github": "https://github.com/gszfwsb/NCFM",
      "url": "https://arxiv.org/abs/2502.20653",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/wang2025ncfm.txt",
      "website": null
    }
  ],
  "2025/02/26":[
    {
      "title": "Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios",
      "author": "Kai Wang & Zekai Li et al., CVPR 2025",
      "github": "https://github.com/NUS-HPC-AI-Lab/EDF",
      "url": "https://arxiv.org/abs/2410.17193",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/wang2025edf.txt",
      "website": null
    },
    {
      "title": "Distilling Long-tailed Datasets",
      "author": "Zhenghao Zhao & Haoxuan Wang et al., CVPR 2025",
      "github": "https://github.com/ichbill/LTDD",
      "url": "https://arxiv.org/abs/2408.14506",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/zhao2025long.txt",
      "website": null
    }
  ],
  "2025/02/15": [
    {
      "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation",
      "author": "Zekai Li & Xinhao Zhong et al., 2025",
      "github": "https://github.com/NUS-HPC-AI-Lab/DD-Ranking",
      "url": null,
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/li2025ranking.txt",
      "website": "https://nus-hpc-ai-lab.github.io/DD-Ranking/"
    },
    {
      "title": "GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost",
      "author": "Xinyi Shang & Peng Sun et al., ICLR 2025",
      "github": "https://github.com/LINs-lab/GIFT",
      "url": "https://arxiv.org/abs/2405.14736",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/shang2025gift.txt",
      "website": null
    }
  ],
  "2025/02/20": [
    {
      "title": "Adaptive Dataset Quantization",
      "author": "Muquan Li et al., AAAI 2025",
      "github": "https://github.com/SLGSP/ADQ",
      "url": "https://www.arxiv.org/abs/2412.16895",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/li2025adq.txt",
      "website": null
    }
  ],
  "2025/02/12": [
    {
      "title": "The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions",
      "author": "Ping Liu et al., 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2502.05673",
      "cite": "https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/liu2025survey.txt",
      "website": null
    }
  ]
}